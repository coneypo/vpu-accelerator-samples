#include <infer_node.hpp>
#include <timeMeasure.hpp>
#include <iostream>

#include <vector>
#include <memory>
#include <string>
#include <ie_compound_blob.h>
#include "stdio.h"
// #include "include/ocv_common.hpp"
// #include "include/classification_results.h"
#include "tinyYolov2_post.h"
#include <ie_iextension.h>
#define HVA_NV12

using namespace InferenceEngine;

InferNode::InferNode(std::size_t inPortNum, std::size_t outPortNum, std::size_t totalThreadNum, const InferInputParams& params)
        :m_params(params), hva::hvaNode_t(inPortNum, outPortNum, totalThreadNum){

}

std::shared_ptr<hva::hvaNodeWorker_t> InferNode::createNodeWorker() const{
    return std::shared_ptr<hva::hvaNodeWorker_t>(new InferNodeWorker((InferNode*)this));
}

InferNodeWorker::InferNodeWorker(hva::hvaNode_t* parentNode)
        :
        hva::hvaNodeWorker_t(parentNode), m_input_height(0), m_input_width(0){

}

void InferNodeWorker::init(){
    auto& param = dynamic_cast<InferNode*>(hva::hvaNodeWorker_t::getParentPtr())->m_params;
    auto& batchingConfig = hva::hvaNodeWorker_t::getParentPtr()->getBatchingConfig();

    // --------------------------- 1. Load inference engine instance -------------------------------------
    ie;
    // ie.AddExtension(std::make_shared<Extensions::Cpu::CpuExtensions>(), "CPU");
    auto extension_ptr = make_so_pointer<IExtension>("libcpu_extension_avx2.so");
    ie.AddExtension(extension_ptr, "CPU");
    // -----------------------------------------------------------------------------------------------------

    // --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
    filenameModel = param.filenameModel; 
    network_reader.ReadNetwork(fileNameToString(filenameModel));
    network_reader.ReadWeights(fileNameToString(filenameModel).substr(0, filenameModel.size() - 4) + ".bin");
    network_reader.getNetwork().setBatchSize(batchingConfig.batchSize);
    network = network_reader.getNetwork();
    // -----------------------------------------------------------------------------------------------------

    // --------------------------- 3. Configure input & output ---------------------------------------------
    // --------------------------- Prepare input blobs -----------------------------------------------------
    input_info = network.getInputsInfo().begin()->second;
    input_name = network.getInputsInfo().begin()->first;

    if (INFER_FORMAT_BGR == param.format) {
        /* Mark input as resizable by setting of a resize algorithm.
            * In this case we will be able to set an input blob of any shape to an infer request.
            * Resize and layout conversions are executed automatically during inference */
        input_info->getPreProcess().setResizeAlgorithm(ResizeAlgorithm::RESIZE_BILINEAR);

        input_info->setLayout(Layout::NHWC);
        input_info->setPrecision(Precision::U8);

        // if (batchingConfig.batchSize == 1) {
        //     preproc = InferNodeWorker::preprocessBGR;
        // } 
        // else if (batchingConfig.batchSize > 1) {
        //     preproc = InferNodeWorker::preprocessBatchBGR;
        // }

    }
    else if (INFER_FORMAT_NV12 == param.format) {
        // set input color format to ColorFormat::NV12 to enable automatic input color format
        // pre-processing        
        input_info->setLayout(Layout::NCHW);
        input_info->setPrecision(Precision::U8);
        // set input resize algorithm to enable input autoresize
        input_info->getPreProcess().setResizeAlgorithm(ResizeAlgorithm::RESIZE_BILINEAR);
        // set input color format to ColorFormat::NV12 to enable automatic input color format
        // pre-processing
        input_info->getPreProcess().setColorFormat(ColorFormat::NV12);
        if (batchingConfig.batchSize == 1) {
            preproc = InferNodeWorker::preprocessNV12;
        } 
        else if (batchingConfig.batchSize > 1) {
            printf("bathing NV12 not supported\n");
            assert(false);
        }
    }
    // --------------------------- Prepare output blobs ----------------------------------------------------
    output_info = network.getOutputsInfo().begin()->second;
    output_name = network.getOutputsInfo().begin()->first;

    output_info->setPrecision(Precision::FP32);
    // -----------------------------------------------------------------------------------------------------

    // --------------------------- 4. Loading model to the device ------------------------------------------
    std::string device_name = "CPU";
    executable_network = ie.LoadNetwork(network, device_name);
    // -----------------------------------------------------------------------------------------------------

    // --------------------------- 5. Create infer request -------------------------------------------------
    infer_request = executable_network.CreateInferRequest();
    // -----------------------------------------------------------------------------------------------------
    
    // set post processing function
    postproc = param.postproc;
}

void InferNodeWorker::process(std::size_t batchIdx){
    vecBlobInput = hvaNodeWorker_t::getParentPtr()->getBatchedInput(batchIdx, std::vector<size_t> {0});
    if(vecBlobInput.size()==0u)
        return;
    preproc(*this);

    // --------------------------- 7. Do inference --------------------------------------------------------
    /* Running the request synchronously */
    infer_request.Infer();
    // -----------------------------------------------------------------------------------------------------
    
    if (nullptr != postproc) {
        postproc(*this);
    }
    else {
        printf("no post proc\n");
    }
}

#if 0
//need modification to fit input
void InferNodeWorker::preprocessBGR(InferNodeWorker& inferWorker) {
    // --------------------------- 6. Prepare input --------------------------------------------------------
    /* Read input image to a blob and set it to an infer request without resize and layout conversions. */
    auto& input_name = inferWorker.input_name;
    auto& infer_request = inferWorker.infer_request;
    cv::Mat image = cv::imread("car.bmp");
    InferenceEngine::Blob::Ptr imgBlob = wrapMat2Blob(image);  // just wrap Mat data by Blob::Ptr without allocating of new memory
    infer_request.SetBlob(input_name, imgBlob);  // infer_request accepts input blob of any size
    // -----------------------------------------------------------------------------------------------------
}

//need modification to fit input
void InferNodeWorker::preprocessBatchBGR(InferNodeWorker& inferWorker) {
    auto& input_name = inferWorker.input_name;
    auto& infer_request = inferWorker.infer_request;
    cv::Mat image = cv::imread("image/car.bmp");
    auto& batchingConfig = inferWorker.getParentPtr()->getBatchingConfig();
    unsigned int batchSize = batchingConfig.batchSize;

    size_t channels = image.channels();
    size_t height = image.size().height;
    size_t width = image.size().width;

    size_t strideH = image.step.buf[0];
    size_t strideW = image.step.buf[1];

    bool is_dense =
            strideW == channels &&
            strideH == channels * width;

    if (!is_dense) THROW_IE_EXCEPTION
                << "Doesn't support conversion from not dense cv::Mat";

    printf("batch size is %d\n", batchSize);

    InferenceEngine::TensorDesc tDesc(InferenceEngine::Precision::U8,
                                      {batchSize, channels, height, width},
                                      InferenceEngine::Layout::NHWC);
    
    auto imageInput = InferenceEngine::make_shared_blob<uint8_t>(tDesc);
    imageInput->allocate();
    unsigned char* data = static_cast<unsigned char*>(imageInput->buffer());
    for (size_t n = 0; n < batchSize; n++) {
        for (size_t h = 0; h < height; h++) {
            for (size_t w = 0; w < width; w++) {
                for (size_t c = 0; c < channels; c++) {
                    data[n * height * width * channels + h * width * channels + w * channels + c] 
                        = image.data[h * width * channels + w * channels + c];
                }
            }
        }
    }
    infer_request.SetBlob(input_name, imageInput);
}

//need modification to fit input
void InferNodeWorker::postprocessClassification(InferNodeWorker& inferWorker) {
    // --------------------------- 8. Process output ------------------------------------------------------
    auto& output_name = inferWorker.output_name;
    auto& infer_request = inferWorker.infer_request;

    auto& batchingConfig = inferWorker.getParentPtr()->getBatchingConfig();
    unsigned int batchSize = batchingConfig.batchSize;

    InferenceEngine::Blob::Ptr output = infer_request.GetBlob(output_name);
    auto desc = output->getTensorDesc();
    auto dims = desc.getDims();
    float* data = static_cast<float*>(output->buffer());


    // Print classification results
    std::vector<std::string> filenames;
    for (int i = 0; i < batchSize; i++){
        filenames.push_back(fileNameToString("car.bmp"));
    }

    ClassificationResult classificationResult(output, filenames, batchSize);
    classificationResult.print();
    // -----------------------------------------------------------------------------------------------------
}
#endif

#if 0
static cv::Mat picNV12; //temporary use for display
#endif

void InferNodeWorker::postprocessTinyYolov2(InferNodeWorker& inferWorker) {

    // --------------------------- 8. Process output ------------------------------------------------------
    auto& output_name = inferWorker.output_name;
    auto& infer_request = inferWorker.infer_request;

    auto& batchingConfig = inferWorker.getParentPtr()->getBatchingConfig();
    unsigned int batchSize = batchingConfig.batchSize;

    const InferenceEngine::Blob::Ptr output_blob = infer_request.GetBlob(output_name);

    std::vector<DetectedObject> vecObjects = YoloV2Tiny::TensorToBBoxYoloV2TinyCommon(output_blob, inferWorker.m_input_height, inferWorker.m_input_width, 0.5, YoloV2Tiny::fillRawNetOut);

    for(auto& object : vecObjects)
    {
        std::cout<<"Object detected:\tx\ty\tw\th"<<std::endl;
        std::cout<<"\t\t\t"<<object.x <<"\t"<<object.y<<"\t"<<object.width<<"\t"<<object.height<<"\n"<<std::endl;
    }

#if 0
    //display
    cv::Mat picBGR;
    cv::cvtColor(picNV12, picBGR, CV_YUV2BGR_NV12);

    for(auto& object : vecObjects)
    {
        cv::rectangle(picBGR, cv::Rect(object.x,object.y, object.width, object.height), cv::Scalar(0,255,0));
    }

    cv::imshow("detection",picBGR);
    cv::waitKey(10);
    // -----------------------------------------------------------------------------------------------------
#endif 
}

void InferNodeWorker::preprocessNV12(InferNodeWorker& inferWorker) {

    // --------------------------- 6. Prepare input --------------------------------------------------------
    /* Read input image to a blob and set it to an infer request without resize and layout conversions. */
    auto& input_name = inferWorker.input_name;
    auto& infer_request = inferWorker.infer_request;
        
// --------------------------- Create a blob to hold the NV12 input data -------------------------------
    // Create tensor descriptors for Y and UV blobs
    
    auto& vecBlobInput = inferWorker.vecBlobInput;
    auto pBuf = vecBlobInput[0]->get<unsigned char, std::pair<unsigned, unsigned>>(0);
    unsigned char* image_buf = pBuf->getPtr();
    std::pair<unsigned,unsigned>* meta = pBuf->getMeta();
    inferWorker.m_input_height = meta->second;
    inferWorker.m_input_width = meta->first;
    InferenceEngine::TensorDesc y_plane_desc(InferenceEngine::Precision::U8,
        {1, 1, inferWorker.m_input_height, inferWorker.m_input_width}, InferenceEngine::Layout::NHWC);
    InferenceEngine::TensorDesc uv_plane_desc(InferenceEngine::Precision::U8,
        {1, 2, inferWorker.m_input_height / 2, inferWorker.m_input_width / 2}, InferenceEngine::Layout::NHWC);
    const size_t offset = inferWorker.m_input_width * inferWorker.m_input_height;
    const size_t sizeY = offset;
    const size_t sizeUV = sizeY / 2;
    const size_t sizeYUV = sizeY + sizeUV;

    // Create blob for Y plane from raw data
    InferenceEngine::Blob::Ptr y_blob = InferenceEngine::make_shared_blob<uint8_t>(y_plane_desc, image_buf);
    // Create blob for UV plane from raw data
    InferenceEngine::Blob::Ptr uv_blob = InferenceEngine::make_shared_blob<uint8_t>(uv_plane_desc, image_buf + offset);
    
    // Create NV12Blob from Y and UV blobs
    InferenceEngine::Blob::Ptr input = make_shared_blob<InferenceEngine::NV12Blob>(y_blob, uv_blob);
#if 0
    picNV12 = cv::Mat(inferWorker.m_input_height * 3/2, inferWorker.m_input_width, CV_8UC1);
    memcpy(picNV12.data, y_blob->buffer(), inferWorker.m_input_height * inferWorker.m_input_width);
    memcpy(picNV12.data + inferWorker.m_input_height * inferWorker.m_input_width, uv_blob->buffer(), inferWorker.m_input_height * inferWorker.m_input_width / 2);
#endif
    // --------------------------- Set the input blob to the InferRequest ----------------------------------
    infer_request.SetBlob(input_name, input);
    // -----------------------------------------------------------------------------------------------------
}

#if 0 //batching NV12 not supported
void InferNodeWorker::preprocessBatchNV12(InferNodeWorker& inferWorker) {
 
    //batching not supported for NV12

    // --------------------------- 6. Prepare input --------------------------------------------------------
    /* Read input image to a blob and set it to an infer request without resize and layout conversions. */
    auto& input_name = inferWorker.input_name;
    auto& infer_request = inferWorker.infer_request;
        
    auto& batchingConfig = inferWorker.getParentPtr()->getBatchingConfig();
    unsigned int batchSize = batchingConfig.batchSize;
// --------------------------- Create a blob to hold the NV12 input data -------------------------------
    // Create tensor descriptors for Y and UV blobs
    const size_t input_height = 720;
    const size_t input_width = 1080;
    
    InferenceEngine::TensorDesc y_plane_desc(InferenceEngine::Precision::U8,
        {batchSize, 1, input_height, input_width}, InferenceEngine::Layout::NHWC);
    InferenceEngine::TensorDesc uv_plane_desc(InferenceEngine::Precision::U8,
        {batchSize, 2, input_height / 2, input_width / 2}, InferenceEngine::Layout::NHWC);
    const size_t offset = input_width * input_height;
    const size_t sizeY = offset;
    const size_t sizeUV = sizeY / 2;
    const size_t sizeYUV = sizeY + sizeUV;


    FILE* fp = fopen("/home/zhangcong/hva/hva_decode_node/build/image/26.nv12", "rb");
    unsigned char* image_buf = (unsigned char*)(malloc(batchSize * sizeYUV));
    // printf("size:%d,fp:%x,buf:%x\n", sizeYUV, fp, image_buf);
    
    // fread(image_buf, sizeof(unsigned char), sizeYUV, fp);
    for (int i = 0; i < batchSize; i++){
        fseek(fp, 0, SEEK_SET);
        fread(image_buf + sizeY * i, sizeof(unsigned char), sizeY, fp);
        fread(image_buf + sizeY * batchSize + sizeUV * i, sizeof(unsigned char), sizeUV, fp);
    }

    fclose(fp);
    // Create blob for Y plane from raw data
    InferenceEngine::Blob::Ptr y_blob = InferenceEngine::make_shared_blob<uint8_t>(y_plane_desc, image_buf);
    // Create blob for UV plane from raw data
    InferenceEngine::Blob::Ptr uv_blob = InferenceEngine::make_shared_blob<uint8_t>(uv_plane_desc, image_buf + offset*batchSize);
    
    // Create NV12Blob from Y and UV blobs
    InferenceEngine::Blob::Ptr input = make_shared_blob<InferenceEngine::NV12Blob>(y_blob, uv_blob);
    
    // --------------------------- Set the input blob to the InferRequest ----------------------------------
    infer_request.SetBlob(input_name, input);
    // -----------------------------------------------------------------------------------------------------
}
#endif